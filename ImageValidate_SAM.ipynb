{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import json\n",
    "import pathlib\n",
    "\n",
    "sys.path.append(\".\")\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import onnxruntime\n",
    "import logging\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "class SegmentAnythingONNX:\n",
    "    \"\"\"Segmentation model using SegmentAnything\"\"\"\n",
    "\n",
    "    def __init__(self, encoder_model_path, decoder_model_path) -> None:\n",
    "        self.target_size = 1024\n",
    "        self.input_size = (684, 1024)\n",
    "\n",
    "        # Load models\n",
    "        providers = onnxruntime.get_available_providers()\n",
    "\n",
    "        # Pop TensorRT Runtime due to crashing issues\n",
    "        # TODO: Add back when TensorRT backend is stable\n",
    "        providers = [p for p in providers if p != \"TensorrtExecutionProvider\"]\n",
    "\n",
    "        if providers:\n",
    "            logging.info(\n",
    "                \"Available providers for ONNXRuntime: %s\", \", \".join(providers)\n",
    "            )\n",
    "        else:\n",
    "            logging.warning(\"No available providers for ONNXRuntime\")\n",
    "        self.encoder_session = onnxruntime.InferenceSession(\n",
    "            encoder_model_path, providers=providers\n",
    "        )\n",
    "        self.encoder_input_name = self.encoder_session.get_inputs()[0].name\n",
    "        self.decoder_session = onnxruntime.InferenceSession(\n",
    "            decoder_model_path, providers=providers\n",
    "        )\n",
    "\n",
    "    def get_input_points(self, prompt):\n",
    "        \"\"\"Get input points\"\"\"\n",
    "        points = []\n",
    "        labels = []\n",
    "        for mark in prompt:\n",
    "            if mark[\"type\"] == \"point\":\n",
    "                points.append(mark[\"data\"])\n",
    "                labels.append(mark[\"label\"])\n",
    "            elif mark[\"type\"] == \"rectangle\":\n",
    "                points.append([mark[\"data\"][0], mark[\"data\"][1]])  # top left\n",
    "                points.append(\n",
    "                    [mark[\"data\"][2], mark[\"data\"][3]]\n",
    "                )  # bottom right\n",
    "                labels.append(2)\n",
    "                labels.append(3)\n",
    "        points, labels = np.array(points), np.array(labels)\n",
    "        return points, labels\n",
    "\n",
    "    def run_encoder(self, encoder_inputs):\n",
    "        \"\"\"Run encoder\"\"\"\n",
    "        output = self.encoder_session.run(None, encoder_inputs)\n",
    "        image_embedding = output[0]\n",
    "        return image_embedding\n",
    "\n",
    "    @staticmethod\n",
    "    def get_preprocess_shape(oldh: int, oldw: int, long_side_length: int):\n",
    "        \"\"\"\n",
    "        Compute the output size given input size and target long side length.\n",
    "        \"\"\"\n",
    "        scale = long_side_length * 1.0 / max(oldh, oldw)\n",
    "        newh, neww = oldh * scale, oldw * scale\n",
    "        neww = int(neww + 0.5)\n",
    "        newh = int(newh + 0.5)\n",
    "        return (newh, neww)\n",
    "\n",
    "    def apply_coords(self, coords: np.ndarray, original_size, target_length):\n",
    "        \"\"\"\n",
    "        Expects a numpy array of length 2 in the final dimension. Requires the\n",
    "        original image size in (H, W) format.\n",
    "        \"\"\"\n",
    "        old_h, old_w = original_size\n",
    "        new_h, new_w = self.get_preprocess_shape(\n",
    "            original_size[0], original_size[1], target_length\n",
    "        )\n",
    "        coords = deepcopy(coords).astype(float)\n",
    "        coords[..., 0] = coords[..., 0] * (new_w / old_w)\n",
    "        coords[..., 1] = coords[..., 1] * (new_h / old_h)\n",
    "        return coords\n",
    "\n",
    "    def run_decoder(\n",
    "        self, image_embedding, original_size, transform_matrix, prompt\n",
    "    ):\n",
    "        \"\"\"Run decoder\"\"\"\n",
    "        input_points, input_labels = self.get_input_points(prompt)\n",
    "\n",
    "        # Add a batch index, concatenate a padding point, and transform.\n",
    "        onnx_coord = np.concatenate(\n",
    "            [input_points, np.array([[0.0, 0.0]])], axis=0\n",
    "        )[None, :, :]\n",
    "        onnx_label = np.concatenate([input_labels, np.array([-1])], axis=0)[\n",
    "            None, :\n",
    "        ].astype(np.float32)\n",
    "        onnx_coord = self.apply_coords(\n",
    "            onnx_coord, self.input_size, self.target_size\n",
    "        ).astype(np.float32)\n",
    "\n",
    "        # Apply the transformation matrix to the coordinates.\n",
    "        onnx_coord = np.concatenate(\n",
    "            [\n",
    "                onnx_coord,\n",
    "                np.ones((1, onnx_coord.shape[1], 1), dtype=np.float32),\n",
    "            ],\n",
    "            axis=2,\n",
    "        )\n",
    "        onnx_coord = np.matmul(onnx_coord, transform_matrix.T)\n",
    "        onnx_coord = onnx_coord[:, :, :2].astype(np.float32)\n",
    "\n",
    "        # Create an empty mask input and an indicator for no mask.\n",
    "        onnx_mask_input = np.zeros((1, 1, 256, 256), dtype=np.float32)\n",
    "        onnx_has_mask_input = np.zeros(1, dtype=np.float32)\n",
    "\n",
    "        decoder_inputs = {\n",
    "            \"image_embeddings\": image_embedding,\n",
    "            \"point_coords\": onnx_coord,\n",
    "            \"point_labels\": onnx_label,\n",
    "            \"mask_input\": onnx_mask_input,\n",
    "            \"has_mask_input\": onnx_has_mask_input,\n",
    "            \"orig_im_size\": np.array(self.input_size, dtype=np.float32),\n",
    "        }\n",
    "        masks, _, _ = self.decoder_session.run(None, decoder_inputs)\n",
    "\n",
    "        # Transform the masks back to the original image size.\n",
    "        inv_transform_matrix = np.linalg.inv(transform_matrix)\n",
    "        transformed_masks = self.transform_masks(\n",
    "            masks, original_size, inv_transform_matrix\n",
    "        )\n",
    "\n",
    "        return transformed_masks\n",
    "\n",
    "    def transform_masks(self, masks, original_size, transform_matrix):\n",
    "        \"\"\"Transform masks\n",
    "        Transform the masks back to the original image size.\n",
    "        \"\"\"\n",
    "        output_masks = []\n",
    "        for batch in range(masks.shape[0]):\n",
    "            batch_masks = []\n",
    "            for mask_id in range(masks.shape[1]):\n",
    "                mask = masks[batch, mask_id]\n",
    "                mask = cv2.warpAffine(\n",
    "                    mask,\n",
    "                    transform_matrix[:2],\n",
    "                    (original_size[1], original_size[0]),\n",
    "                    flags=cv2.INTER_LINEAR,\n",
    "                )\n",
    "                batch_masks.append(mask)\n",
    "            output_masks.append(batch_masks)\n",
    "        return np.array(output_masks)\n",
    "\n",
    "    def encode(self, cv_image):\n",
    "        \"\"\"\n",
    "        Calculate embedding and metadata for a single image.\n",
    "        \"\"\"\n",
    "        original_size = cv_image.shape[:2]\n",
    "\n",
    "        # Calculate a transformation matrix to convert to self.input_size\n",
    "        scale_x = self.input_size[1] / cv_image.shape[1]\n",
    "        scale_y = self.input_size[0] / cv_image.shape[0]\n",
    "        scale = min(scale_x, scale_y)\n",
    "        transform_matrix = np.array(\n",
    "            [\n",
    "                [scale, 0, 0],\n",
    "                [0, scale, 0],\n",
    "                [0, 0, 1],\n",
    "            ]\n",
    "        )\n",
    "        cv_image = cv2.warpAffine(\n",
    "            cv_image,\n",
    "            transform_matrix[:2],\n",
    "            (self.input_size[1], self.input_size[0]),\n",
    "            flags=cv2.INTER_LINEAR,\n",
    "        )\n",
    "\n",
    "        encoder_inputs = {\n",
    "            self.encoder_input_name: cv_image.astype(np.float32),\n",
    "        }\n",
    "        image_embedding = self.run_encoder(encoder_inputs)\n",
    "        return {\n",
    "            \"image_embedding\": image_embedding,\n",
    "            \"original_size\": original_size,\n",
    "            \"transform_matrix\": transform_matrix,\n",
    "        }\n",
    "\n",
    "    def predict_masks(self, embedding, prompt):\n",
    "        \"\"\"\n",
    "        Predict masks for a single image.\n",
    "        \"\"\"\n",
    "        masks = self.run_decoder(\n",
    "            embedding[\"image_embedding\"],\n",
    "            embedding[\"original_size\"],\n",
    "            embedding[\"transform_matrix\"],\n",
    "            prompt,\n",
    "        )\n",
    "\n",
    "        return masks\n",
    "\n",
    "\n",
    "def str2bool(v):\n",
    "    return v.lower() in (\"true\", \"1\")\n",
    "\n",
    "\n",
    "argparser = argparse.ArgumentParser()\n",
    "argparser.add_argument(\n",
    "    \"--encoder_model\",\n",
    "    type=str,\n",
    "    default=\"sam_vit_b.encoder.onnx\",\n",
    "    help=\"Path to the ONNX encoder model\",\n",
    ")\n",
    "argparser.add_argument(\n",
    "    \"--decoder_model\",\n",
    "    type=str,\n",
    "    default=\"sam_vit_b.decoder.onnx\",\n",
    "    help=\"Path to the ONNX decoder model\",\n",
    ")\n",
    "argparser.add_argument(\n",
    "    \"--image\",\n",
    "    type=str,\n",
    "    default=\"Images/truck.jpg\",\n",
    "    help=\"Path to the image\",\n",
    ")\n",
    "argparser.add_argument(\n",
    "    \"--prompt\",\n",
    "    type=str,\n",
    "    default=\"Images/truck.json\",\n",
    "    help=\"Path to the image\",\n",
    ")\n",
    "argparser.add_argument(\n",
    "    \"--output\",\n",
    "    type=str,\n",
    "    default=None,\n",
    "    help=\"Path to the output image\",\n",
    ")\n",
    "argparser.add_argument(\n",
    "    \"--show\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Show the result\",\n",
    ")\n",
    "argparser.add_argument(\n",
    "    \"--sam_variant\",\n",
    "    type=str,\n",
    "    default=\"sam\",\n",
    "    help=\"Variant of SAM model. Options: sam\",\n",
    ")\n",
    "args = argparser.parse_args()\n",
    "\n",
    "model = None\n",
    "if args.sam_variant == \"sam\":\n",
    "    model = SegmentAnythingONNX(\n",
    "        args.encoder_model,\n",
    "        args.decoder_model,\n",
    "    )\n",
    "\n",
    "image = cv2.imread(args.image)\n",
    "prompt = json.load(open(args.prompt))\n",
    "\n",
    "embedding = model.encode(image)\n",
    "\n",
    "masks = model.predict_masks(embedding, prompt)\n",
    "\n",
    "# Merge masks\n",
    "mask = np.zeros((masks.shape[2], masks.shape[3], 3), dtype=np.uint8)\n",
    "for m in masks[0, :, :, :]:\n",
    "    mask[m > 0.5] = [255, 0, 0]\n",
    "\n",
    "# Binding image and mask\n",
    "visualized = cv2.addWeighted(image, 0.5, mask, 0.5, 0)\n",
    "\n",
    "# Draw the prompt points and rectangles.\n",
    "for p in prompt:\n",
    "    if p[\"type\"] == \"point\":\n",
    "        color = (\n",
    "            (0, 255, 0) if p[\"label\"] == 1 else (0, 0, 255)\n",
    "        )  # green for positive, red for negative\n",
    "        cv2.circle(visualized, (p[\"data\"][0], p[\"data\"][1]), 10, color, -1)\n",
    "    elif p[\"type\"] == \"rectangle\":\n",
    "        cv2.rectangle(\n",
    "            visualized,\n",
    "            (p[\"data\"][0], p[\"data\"][1]),\n",
    "            (p[\"data\"][2], p[\"data\"][3]),\n",
    "            (0, 255, 0),\n",
    "            2,\n",
    "        )\n",
    "\n",
    "if args.output is not None:\n",
    "    pathlib.Path(args.output).parent.mkdir(parents=True, exist_ok=True)\n",
    "    cv2.imwrite(args.output, visualized)\n",
    "else:\n",
    "    cv2.imwrite(\"result.jpg\", visualized)\n",
    "\n",
    "if args.show:\n",
    "    cv2.imshow(\"Result\", visualized)\n",
    "    cv2.waitKey(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
