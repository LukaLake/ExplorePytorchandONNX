{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 将Yolo模型转化为ONNX格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the YOLOv8 model\n",
    "model = YOLO('yolo11m-seg.pt')\n",
    "\n",
    "# Move the model to CPU\n",
    "model.to('cpu')\n",
    "\n",
    "# Export the model to ONNX format with static input shapes and without simplification\n",
    "model.export(\n",
    "    format='onnx',\n",
    "    opset=12,\n",
    "    simplify=False,\n",
    "    dynamic=False,\n",
    ")\n",
    "\n",
    "print(\"Model successfully converted to ONNX format.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 将SAM (Segment Anything) 导出为ONNX格式\n",
    "还是需要使用官方提供的脚本\n",
    "**但是并不包含Encoder？**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from segment_anything import sam_model_registry\n",
    "from segment_anything.utils.onnx import SamOnnxModel\n",
    "import onnxruntime\n",
    "\n",
    "def export_sam_to_onnx(checkpoint=\"sam_vit_b.pth\", output=\"sam_model.onnx\", model_type=\"vit_b\", opset=17,\n",
    "                       return_single_mask=True, use_stability_score=False, return_extra_metrics=False):\n",
    "    print(\"Loading SAM model...\")\n",
    "    # 加载模型\n",
    "    sam = sam_model_registry[model_type](checkpoint=checkpoint)\n",
    "    onnx_model = SamOnnxModel(\n",
    "        model=sam,\n",
    "        return_single_mask=return_single_mask,\n",
    "        use_stability_score=use_stability_score,\n",
    "        return_extra_metrics=return_extra_metrics,\n",
    "    )\n",
    "\n",
    "    # 设置动态维度\n",
    "    dynamic_axes = {\n",
    "        \"point_coords\": {1: \"num_points\"},\n",
    "        \"point_labels\": {1: \"num_points\"},\n",
    "    }\n",
    "\n",
    "    # 定义虚拟输入数据\n",
    "    embed_dim = sam.prompt_encoder.embed_dim\n",
    "    embed_size = sam.prompt_encoder.image_embedding_size\n",
    "    mask_input_size = [4 * x for x in embed_size]\n",
    "    dummy_inputs = {\n",
    "        \"image_embeddings\": torch.randn(1, embed_dim, *embed_size, dtype=torch.float),\n",
    "        \"point_coords\": torch.randn(1, 5, 2, dtype=torch.float),\n",
    "        \"point_labels\": torch.randn(1, 5, dtype=torch.float),\n",
    "        \"mask_input\": torch.randn(1, 1, *mask_input_size, dtype=torch.float),\n",
    "        \"has_mask_input\": torch.tensor([1.0], dtype=torch.float),\n",
    "        \"orig_im_size\": torch.tensor([1500, 2250], dtype=torch.float),\n",
    "    }\n",
    "\n",
    "    output_names = [\"masks\", \"iou_predictions\", \"low_res_masks\"]\n",
    "\n",
    "    # 导出模型为 ONNX 格式\n",
    "    print(f\"Exporting ONNX model to {output}...\")\n",
    "    torch.onnx.export(\n",
    "        onnx_model,\n",
    "        tuple(dummy_inputs.values()),\n",
    "        output,\n",
    "        export_params=True, # 是否导出模型参数\n",
    "        verbose=False, # 是否打印详细信息\n",
    "        do_constant_folding=True, # 是否执行常量折叠\n",
    "        opset_version=opset,\n",
    "        input_names=list(dummy_inputs.keys()),\n",
    "        output_names=output_names,\n",
    "        dynamic_axes=dynamic_axes,\n",
    "    )\n",
    "    print(\"Model exported to ONNX format successfully.\")\n",
    "\n",
    "    # 使用 ONNXRuntime 测试导出的模型\n",
    "    ort_session = onnxruntime.InferenceSession(output)\n",
    "    ort_inputs = {k: v.cpu().numpy() for k, v in dummy_inputs.items()}\n",
    "    _ = ort_session.run(None, ort_inputs)\n",
    "    print(\"ONNX model successfully tested with ONNXRuntime.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 直接调用导出函数并指定默认值\n",
    "    export_sam_to_onnx()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导出完整SAM（包含Encoder）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用samexporter输出encoder部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import onnx\n",
    "\n",
    "from segment_anything import sam_model_registry\n",
    "from samexporter.mobile_encoder.setup_mobile_sam import setup_model\n",
    "from samexporter.onnx_utils import ImageEncoderOnnxModel\n",
    "from onnx.external_data_helper import convert_model_to_external_data\n",
    "\n",
    "import os\n",
    "from tempfile import mkdtemp\n",
    "import pathlib\n",
    "import shutil\n",
    "import argparse\n",
    "import warnings\n",
    "\n",
    "parser = argparse.ArgumentParser(\n",
    "    description=\"Export the SAM image encoder to an ONNX model.\"\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--checkpoint\",\n",
    "    type=str,\n",
    "    required=True,\n",
    "    help=\"The path to the SAM model checkpoint.\",\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--output\",\n",
    "    type=str,\n",
    "    required=True,\n",
    "    help=\"The filename to save the ONNX model to.\",\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--model-type\",\n",
    "    type=str,\n",
    "    required=True,\n",
    "    help=\"In ['default', 'vit_h', 'vit_l', 'vit_b', 'mobile']. \"\n",
    "    \"Which type of SAM model to export.\",\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--use-preprocess\",\n",
    "    action=\"store_true\",\n",
    "    help=(\"Embed pre-processing into the model\",),\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--opset\",\n",
    "    type=int,\n",
    "    default=17,\n",
    "    help=\"The ONNX opset version to use. Must be >=11\",\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--quantize-out\",\n",
    "    type=str,\n",
    "    default=None,\n",
    "    help=(\n",
    "        \"If set, will quantize the model and save it with this name. \"\n",
    "        \"Quantization is performed with quantize_dynamic from \"\n",
    "        \"onnxruntime.quantization.quantize.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--gelu-approximate\",\n",
    "    action=\"store_true\",\n",
    "    help=(\n",
    "        \"Replace GELU operations with approximations using tanh. Useful \"\n",
    "        \"for some runtimes that have slow or unimplemented erf ops, used in GELU.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "def run_export(\n",
    "    model_type: str,\n",
    "    checkpoint: str,\n",
    "    output: str,\n",
    "    use_preprocess: bool,\n",
    "    opset: int,\n",
    "    gelu_approximate: bool = False,\n",
    "):\n",
    "    print(\"Loading model...\")\n",
    "    if model_type == \"mobile\":\n",
    "        checkpoint = torch.load(checkpoint, map_location=\"cpu\")\n",
    "        sam = setup_model()\n",
    "        sam.load_state_dict(checkpoint, strict=True)\n",
    "    else:\n",
    "        sam = sam_model_registry[model_type](checkpoint=checkpoint)\n",
    "\n",
    "    onnx_model = ImageEncoderOnnxModel(\n",
    "        model=sam,\n",
    "        use_preprocess=use_preprocess,\n",
    "        pixel_mean=[123.675, 116.28, 103.53],\n",
    "        pixel_std=[58.395, 57.12, 57.375],\n",
    "    )\n",
    "\n",
    "    if gelu_approximate:\n",
    "        for _, m in onnx_model.named_modules():\n",
    "            if isinstance(m, torch.nn.GELU):\n",
    "                m.approximate = \"tanh\"\n",
    "\n",
    "    image_size = sam.image_encoder.img_size\n",
    "    if use_preprocess:\n",
    "        dummy_input = {\n",
    "            \"input_image\": torch.randn(\n",
    "                (image_size, image_size, 3), dtype=torch.float\n",
    "            )\n",
    "        }\n",
    "        dynamic_axes = {\n",
    "            \"input_image\": {0: \"image_height\", 1: \"image_width\"},\n",
    "        }\n",
    "    else:\n",
    "        dummy_input = {\n",
    "            \"input_image\": torch.randn(\n",
    "                (1, 3, image_size, image_size), dtype=torch.float\n",
    "            )\n",
    "        }\n",
    "        dynamic_axes = None\n",
    "\n",
    "    _ = onnx_model(**dummy_input)\n",
    "\n",
    "    output_names = [\"image_embeddings\"]\n",
    "\n",
    "    onnx_base = os.path.splitext(os.path.basename(output))[0]\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=torch.jit.TracerWarning)\n",
    "        warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "        print(f\"Exporting onnx model to {output}...\")\n",
    "        if model_type == \"vit_h\":\n",
    "            tmp_dir = mkdtemp()\n",
    "            tmp_model_path = os.path.join(tmp_dir, f\"{onnx_base}.onnx\")\n",
    "            torch.onnx.export(\n",
    "                onnx_model,\n",
    "                tuple(dummy_input.values()),\n",
    "                tmp_model_path,\n",
    "                export_params=True,\n",
    "                verbose=False,\n",
    "                opset_version=opset,\n",
    "                do_constant_folding=True,\n",
    "                input_names=list(dummy_input.keys()),\n",
    "                output_names=output_names,\n",
    "                dynamic_axes=dynamic_axes,\n",
    "            )\n",
    "\n",
    "            # Combine the weights into a single file\n",
    "            pathlib.Path(output).parent.mkdir(parents=True, exist_ok=True)\n",
    "            onnx_model = onnx.load(tmp_model_path)\n",
    "            convert_model_to_external_data(\n",
    "                onnx_model,\n",
    "                all_tensors_to_one_file=True,\n",
    "                location=f\"{onnx_base}_data.bin\",\n",
    "                size_threshold=1024,\n",
    "                convert_attribute=False,\n",
    "            )\n",
    "\n",
    "            # Save the model\n",
    "            onnx.save(onnx_model, output)\n",
    "\n",
    "            # Cleanup the temporary directory\n",
    "            shutil.rmtree(tmp_dir)\n",
    "        else:\n",
    "            with open(output, \"wb\") as f:\n",
    "                torch.onnx.export(\n",
    "                    onnx_model,\n",
    "                    tuple(dummy_input.values()),\n",
    "                    f,\n",
    "                    export_params=True,\n",
    "                    verbose=False,\n",
    "                    opset_version=opset,\n",
    "                    do_constant_folding=True,\n",
    "                    input_names=list(dummy_input.keys()),\n",
    "                    output_names=output_names,\n",
    "                    dynamic_axes=dynamic_axes,\n",
    "                )\n",
    "\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.cpu().numpy()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parser.parse_args()\n",
    "    run_export(\n",
    "        model_type=args.model_type,\n",
    "        checkpoint=args.checkpoint,\n",
    "        output=args.output,\n",
    "        use_preprocess=args.use_preprocess,\n",
    "        opset=args.opset,\n",
    "        gelu_approximate=args.gelu_approximate,\n",
    "    )\n",
    "\n",
    "    if args.quantize_out is not None:\n",
    "        from onnxruntime.quantization import QuantType  # type: ignore\n",
    "        from onnxruntime.quantization.quantize import quantize_dynamic  # type: ignore\n",
    "\n",
    "        print(f\"Quantizing model and writing to {args.quantize_out}...\")\n",
    "        quantize_dynamic(\n",
    "            model_input=args.output,\n",
    "            model_output=args.quantize_out,\n",
    "            per_channel=False,\n",
    "            reduce_range=False,\n",
    "            weight_type=QuantType.QUInt8,\n",
    "        )\n",
    "        print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用samexport输出decoder部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# All rights reserved.\n",
    "\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\n",
    "import pathlib\n",
    "\n",
    "import torch\n",
    "\n",
    "from segment_anything import sam_model_registry\n",
    "from segment_anything.utils.onnx import SamOnnxModel\n",
    "\n",
    "import argparse\n",
    "import warnings\n",
    "\n",
    "try:\n",
    "    import onnxruntime  # type: ignore\n",
    "\n",
    "    onnxruntime_exists = True\n",
    "except ImportError:\n",
    "    onnxruntime_exists = False\n",
    "\n",
    "parser = argparse.ArgumentParser(\n",
    "    description=\"Export the SAM prompt encoder and mask decoder to an ONNX model.\"\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--checkpoint\",\n",
    "    type=str,\n",
    "    required=True,\n",
    "    help=\"The path to the SAM model checkpoint.\",\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--output\",\n",
    "    type=str,\n",
    "    required=True,\n",
    "    help=\"The filename to save the ONNX model to.\",\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--model-type\",\n",
    "    type=str,\n",
    "    required=True,\n",
    "    help=\"In ['default', 'vit_h', 'vit_l', 'vit_b']. \"\n",
    "    \"Which type of SAM model to export.\",\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--return-single-mask\",\n",
    "    action=\"store_true\",\n",
    "    help=(\n",
    "        \"If true, the exported ONNX model will only return the best mask, \"\n",
    "        \"instead of returning multiple masks. For high resolution images \"\n",
    "        \"this can improve runtime when upscaling masks is expensive.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--opset\",\n",
    "    type=int,\n",
    "    default=17,\n",
    "    help=\"The ONNX opset version to use. Must be >=11\",\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--quantize-out\",\n",
    "    type=str,\n",
    "    default=None,\n",
    "    help=(\n",
    "        \"If set, will quantize the model and save it with this name. \"\n",
    "        \"Quantization is performed with quantize_dynamic from \"\n",
    "        \"onnxruntime.quantization.quantize.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--gelu-approximate\",\n",
    "    action=\"store_true\",\n",
    "    help=(\n",
    "        \"Replace GELU operations with approximations using tanh. Useful \"\n",
    "        \"for some runtimes that have slow or unimplemented erf ops, used in GELU.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--use-stability-score\",\n",
    "    action=\"store_true\",\n",
    "    help=(\n",
    "        \"Replaces the model's predicted mask quality score with the stability \"\n",
    "        \"score calculated on the low resolution masks using an offset of 1.0. \"\n",
    "    ),\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--return-extra-metrics\",\n",
    "    action=\"store_true\",\n",
    "    help=(\n",
    "        \"The model will return five results: (masks, scores, stability_scores, \"\n",
    "        \"areas, low_res_logits) instead of the usual three. This can be \"\n",
    "        \"significantly slower for high resolution outputs.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "def run_export(\n",
    "    model_type: str,\n",
    "    checkpoint: str,\n",
    "    output: str,\n",
    "    opset: int,\n",
    "    return_single_mask: bool,\n",
    "    gelu_approximate: bool = False,\n",
    "    use_stability_score: bool = False,\n",
    "    return_extra_metrics=False,\n",
    "):\n",
    "    print(\"Loading model...\")\n",
    "    sam = sam_model_registry[model_type](checkpoint=checkpoint)\n",
    "\n",
    "    onnx_model = SamOnnxModel(\n",
    "        model=sam,\n",
    "        return_single_mask=return_single_mask,\n",
    "        use_stability_score=use_stability_score,\n",
    "        return_extra_metrics=return_extra_metrics,\n",
    "    )\n",
    "\n",
    "    if gelu_approximate:\n",
    "        for _, m in onnx_model.named_modules():\n",
    "            if isinstance(m, torch.nn.GELU):\n",
    "                m.approximate = \"tanh\"\n",
    "\n",
    "    dynamic_axes = {\n",
    "        \"point_coords\": {1: \"num_points\"},\n",
    "        \"point_labels\": {1: \"num_points\"},\n",
    "    }\n",
    "\n",
    "    embed_dim = sam.prompt_encoder.embed_dim\n",
    "    embed_size = sam.prompt_encoder.image_embedding_size\n",
    "    mask_input_size = [4 * x for x in embed_size]\n",
    "    dummy_inputs = {\n",
    "        \"image_embeddings\": torch.randn(\n",
    "            1, embed_dim, *embed_size, dtype=torch.float\n",
    "        ),\n",
    "        \"point_coords\": torch.randint(\n",
    "            low=0, high=1024, size=(1, 5, 2), dtype=torch.float\n",
    "        ),\n",
    "        \"point_labels\": torch.randint(\n",
    "            low=0, high=4, size=(1, 5), dtype=torch.float\n",
    "        ),\n",
    "        \"mask_input\": torch.randn(1, 1, *mask_input_size, dtype=torch.float),\n",
    "        \"has_mask_input\": torch.tensor([1], dtype=torch.float),\n",
    "        \"orig_im_size\": torch.tensor([1500, 2250], dtype=torch.float),\n",
    "    }\n",
    "\n",
    "    _ = onnx_model(**dummy_inputs)\n",
    "\n",
    "    output_names = [\"masks\", \"iou_predictions\", \"low_res_masks\"]\n",
    "\n",
    "    pathlib.Path(output).parent.mkdir(parents=True, exist_ok=True)\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=torch.jit.TracerWarning)\n",
    "        warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "        with open(output, \"wb\") as f:\n",
    "            print(f\"Exporting onnx model to {output}...\")\n",
    "            torch.onnx.export(\n",
    "                onnx_model,\n",
    "                tuple(dummy_inputs.values()),\n",
    "                f,\n",
    "                export_params=True,\n",
    "                verbose=False,\n",
    "                opset_version=opset,\n",
    "                do_constant_folding=True,\n",
    "                input_names=list(dummy_inputs.keys()),\n",
    "                output_names=output_names,\n",
    "                dynamic_axes=dynamic_axes,\n",
    "            )\n",
    "\n",
    "    if onnxruntime_exists:\n",
    "        ort_inputs = {k: to_numpy(v) for k, v in dummy_inputs.items()}\n",
    "        # set cpu provider default\n",
    "        providers = [\"CPUExecutionProvider\"]\n",
    "        ort_session = onnxruntime.InferenceSession(output, providers=providers)\n",
    "        _ = ort_session.run(None, ort_inputs)\n",
    "        print(\"Model has successfully been run with ONNXRuntime.\")\n",
    "\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.cpu().numpy()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parser.parse_args()\n",
    "    run_export(\n",
    "        model_type=args.model_type,\n",
    "        checkpoint=args.checkpoint,\n",
    "        output=args.output,\n",
    "        opset=args.opset,\n",
    "        return_single_mask=args.return_single_mask,\n",
    "        gelu_approximate=args.gelu_approximate,\n",
    "        use_stability_score=args.use_stability_score,\n",
    "        return_extra_metrics=args.return_extra_metrics,\n",
    "    )\n",
    "\n",
    "    if args.quantize_out is not None:\n",
    "        assert (\n",
    "            onnxruntime_exists\n",
    "        ), \"onnxruntime is required to quantize the model.\"\n",
    "        from onnxruntime.quantization import QuantType  # type: ignore\n",
    "        from onnxruntime.quantization.quantize import quantize_dynamic  # type: ignore\n",
    "\n",
    "        print(f\"Quantizing model and writing to {args.quantize_out}...\")\n",
    "        quantize_dynamic(\n",
    "            model_input=args.output,\n",
    "            model_output=args.quantize_out,\n",
    "            per_channel=False,\n",
    "            reduce_range=False,\n",
    "            weight_type=QuantType.QUInt8,\n",
    "        )\n",
    "        print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python -m samexporter.export_encoder --checkpoint sam_vit_h.pth --output sam_vit_h.encoder.onnx --model-type vit_b --use-preprocess\n",
    "python -m samexporter.export_decoder --checkpoint sam_vit_h.pth --output sam_vit_h.decoder.onnx --model-type vit_b\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
